# 简介

在集群环境中，附件（照片）存储是一个麻烦的事情，不能存储在集群的任一台服务器上，只能集中存储（静态资源服务器），这样任何一台集群服务器都可以访问。  
特点：  
* 照片很少修改，修改的话系统默认存为一个新的文件  
* 照片不会很大，基本上是10K~5M  
* 照片的高可用性，需要做冗余存储  
在看了GFS的论文后，手痒想写一个自己使用的分布式文件存储，同时移动硬盘损坏，让我很纠结，突然想写类似网盘的应用，这个项目先实现基础的文件存储。  

# 系统设计

nameserver:存储文件的元数据（内存），汇总各个dataserver的状态。  
dataserver:存储文件实际的数据，和客户端交互，处理读写操作。  
客户端：发送存储和读取文件的操作    

### 最小存储单元   
 
采用64M的block存储数据（实际大小65M，预留1M存储元素据），每个block有一个唯一的blockId，相同的block会冗余存储在不同的dataserver上，相同ID的block原则上必须保证，里面的数据和元数据完全相同。当出现异常，舍去掉该台的block,从正确的服务器上拷贝（目前这块还没有实现）。基本上参照GFS的设计。blockId为一个int类型递增数，block的文件名用blockId命名。

### 文件命名格式  

SFS下的文件，都有一个唯一命名格式，用于读取。  
4位blockId+UUID的Hash值(long 8位)+“.”+文件扩展名。12位的byte,再用Base58加密成字符串。

### 网络传输

网络传输采用Netty框架，简单用注解方式封装了传输协议和处理逻辑。nameserver,dataserver之间，采用长连接，客户端和nameserver,dataserver都采用短连接。dataserver之间的文件传送也采用短连接。  

### 文件写流程  
1.客户端向nameserver获取在线DataServer  

2.nameserver从在线的DataServer中，选举出一个leader,负责保证所有block都被成功写  

3.从leader中找到一个可写的block,找到这个block关联的所有dataserver返回给客户端，并且锁定该block  

4.如果leader没有可写的block,则创建一个新的Block(几台在线的dataserver同时创建)  

5.客户端向leader（dataserver）,分包发送数据文件。  

6.leader成功收到一个分包，同时向其他包含该block的包发送分包。当所有分包传送完毕，更新本地的索引信息。同时向nameserver发送元数据信息。中间所有过程出错，都会回滚，保证数据完整性。  

7.客户端确认所有分包发送完成，结束写流程，并向nameserver释放文件锁。

### 文件读流程
1.客户端向nameserver发送查询信息，返回文件的元数据信息（包括blockId,索引位置）和 在线包含该block的Dataserver信息。  
2.客户端随即获取一个DataServer读取文件数据。

### block结构

8字节（数据文件位置）+64M数据文件+8字节（元数据位置）+1M的元数据   
每个文件的元数据包括：8位文件名+4位扩展名+8位数据文件索引+8位文件长度
 
  
